{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 벨만 방정식 (Bellman Equation)\n",
    "\n",
    "벨만 방정식은 강화학습에서 가치 함수(State Value Function 또는 Action Value Function)를 정의하는 핵심 방정식입니다. 이 방정식은 현재 상태의 가치를 그 상태에서 가능한 행동을 통해 얻는 보상과 다음 상태에서의 가치의 합으로 표현합니다. 벨만 방정식은 강화학습 알고리즘의 이론적 기초로 사용됩니다.\n",
    "\n",
    "## `상태 가치 함수`에 대한 벨만 방정식\n",
    "상태 가치 함수 $V^{\\pi}(s)$는 정책 $\\pi$ 하에서 상태 $s$에 있을 때 미래에 받을 총 보상의 기대값입니다. 벨만 방정식은 이 함수가 재귀적으로 정의될 수 있음을 보여줍니다.\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\sum_{a} \\pi(a | s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^{\\pi}(s') \\right]\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "- $V^{\\pi}(s)$: 정책 $\\pi$ 하에서 상태 $s$의 가치.\n",
    "- $\\pi(a | s)$: 상태 $s$에서 행동 $a$를 선택할 확률.\n",
    "- $P(s' | s, a)$: 상태 $s$에서 행동 $a$를 수행한 후 상태 $s'$로 전이될 확률.\n",
    "- $R(s, a, s')$: 상태 $s$에서 행동 $a$를 취해 상태 $s'$로 전이될 때 받는 보상.\n",
    "- $\\gamma$: 할인율 (0 ≤ $\\gamma$ < 1), 미래 보상의 현재 가치 반영.\n",
    "\n",
    "`즉, 현재 상태에서 모든 가능한 행동을 고려했을 때 그 행동의 결과로 도달할 다음 상태들에서의 기대 보상을 계산하여 합산한 것입니다.\n",
    "따라서 이 식은 행동 확률에 따른 각 행동의 다음 단계에서 기대되는 보상값들의 총합`\n",
    "\n",
    "\n",
    "## `행동 가치 함수`에 대한 벨만 방정식\n",
    "행동 가치 함수 $Q^{\\pi}(s, a)$는 정책 $\\pi$ 하에서 상태 $s$에서 행동 $a$를 선택했을 때의 기대되는 총 보상입니다.\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) = \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma \\sum_{a'} \\pi(a' | s') Q^{\\pi}(s', a') \\right]\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "- $Q^{\\pi}(s, a)$: 정책 $\\pi$ 하에서 상태 $s$에서 행동 $a$를 선택했을 때의 가치.\n",
    "- $a'$: 다음 상태 $s'$에서 선택할 수 있는 모든 가능한 행동.\n",
    "\n",
    "## `최적 가치 함수`에 대한 벨만 최적 방정식\n",
    "최적 정책 $\\pi^*$는 에이전트가 최대한의 보상을 얻을 수 있도록 하는 정책입니다. 최적 상태 가치 함수 $V^*(s)$는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a} \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^*(s') \\right]\n",
    "$$\n",
    "\n",
    "마찬가지로, 최적 행동 가치 함수 $Q^*(s, a)$는 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "Q^*(s, a) = \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma \\max_{a'} Q^*(s', a') \\right]\n",
    "$$\n",
    "\n",
    "## 벨만 방정식의 중요성\n",
    "- **정책 평가**: 벨만 방정식은 주어진 정책의 상태 가치 함수를 평가할 수 있게 합니다.\n",
    "- **정책 개선**: 최적 정책을 찾기 위한 값 반복(Value Iteration) 및 정책 반복(Policy Iteration)과 같은 알고리즘에서 사용됩니다.\n",
    "- **재귀적 정의**: 상태의 가치를 현재와 미래의 보상에 대해 재귀적으로 표현하므로, 강화학습 문제를 동적 계획법의 형태로 접근할 수 있게 합니다.\n",
    "\n",
    "벨만 방정식은 강화학습의 이론적 기반으로, 에이전트가 최적의 행동과 경로를 학습할 수 있는 중요한 기초를 제공합니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
