{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s'=f(s,a)$\n",
    "\n",
    "$p(s'|s,a)$ 상태전의 함수: 는 다음 상태로 갈 확률\n",
    "\n",
    "s: 현재 상태\n",
    "a: 어떤 행동\n",
    "\n",
    "$r(s=L2,a=Left,s'=L1)=1$ 보상함수\n",
    "\n",
    "- $ α = μ(s) $ 정책 함수: 에이전트가 특정 상태 $ s $에서 어떤 행동을 할 확률을 결정하는 **확률적 함수**입니다.\n",
    "- 예를 들어, 특정 상태 $ s = L3 $에서 에이전트가 행동 $ \\text{Right} $를 선택할 확률이 0.4인 경우는 다음과 같이 표현할 수 있습니다:\n",
    "\n",
    "$$\n",
    "π(α = \\text{Right} | s = L3) = 0.4\n",
    "$$\n",
    "\n",
    "- 이 수식은 정책 함수 $π$가 상태 $ s = L3 $에서 행동 $\\text{Right}$를 취할 확률이 0.4임을 의미합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 보상의 할인율과 지수 가중치 사용 이유\n",
    "\n",
    "강화학습에서 보상의 할인율은 미래 보상의 현재 가치를 계산할 때 사용됩니다. 일반적으로 **지수 가중치**를 사용하는 이유는 시간이 지남에 따라 미래 보상의 중요도를 점진적으로 감소시키기 위해서입니다. 이 방식은 멀리 있는 미래 보상보다 가까운 보상을 더 중요하게 취급하게 만듭니다.\n",
    "\n",
    "## 할인된 보상의 수식\n",
    "강화학습에서 $ t $ 시점의 보상을 $ R_t $라고 할 때, 할인율 $ \\gamma $ (0 ≤ $ \\gamma $ < 1)를 사용하여 현재 시점에서의 총 보상을 계산하는 식은 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k}\n",
    "$$\n",
    "\n",
    "## 지수 가중치를 사용하는 이유\n",
    "- **지수적 감소**: $\\gamma$를 반복적으로 곱하여 보상의 중요도를 지수적으로 감소시킵니다. 이렇게 함으로써 시간이 지남에 따라 보상의 가치가 점차 줄어듭니다.\n",
    "- **수학적 단순성**: 지수 가중치를 사용하면 계산이 간단해지고 수렴성을 보장할 수 있습니다. 이는 보상 합계의 수렴 성질을 갖게 하여 총 보상의 유한성을 보장합니다.\n",
    "- **미래 보상에 대한 신뢰도**: 현실적으로 먼 미래의 보상은 불확실성이 커지기 때문에, 할인율을 통해 미래 보상의 가치를 점진적으로 줄이는 것이 더 합리적입니다.\n",
    "\n",
    "## 예시\n",
    "할인율 $\\gamma = 0.9$인 경우, 각 시간 단계에서의 가중치는 다음과 같습니다:\n",
    "- 현재 보상 $R_t$: 가중치 1 ($\\gamma^0$)\n",
    "- 다음 단계 보상 $R_{t+1}$: 가중치 0.9 ($\\gamma^1$)\n",
    "- 그다음 단계 보상 $R_{t+2}$: 가중치 0.81 ($\\gamma^2$)\n",
    "- 등등.\n",
    "\n",
    "이렇게 계산된 총 보상 $G_t$는 **가중치를 고려한 미래 보상의 합**으로, 에이전트가 최적의 정책을 학습할 수 있도록 도와줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상태 가치 함수 (State Value Function)\n",
    "\n",
    "상태 가치 함수는 강화학습에서 매우 중요한 개념으로, 특정 상태에서 시작하여 미래에 받을 수 있는 총 보상의 기대값을 나타냅니다. 이 함수는 에이전트가 현재 상태에서 앞으로 선택할 행동과 환경의 반응을 고려하여 계산됩니다.\n",
    "\n",
    "## 정의\n",
    "상태 가치 함수 $V(s)$는 주어진 정책 $\\pi$ 하에서 특정 상태 $s$에 있을 때, 미래에 받을 것으로 기대되는 **할인된 총 보상**의 기대값입니다. 이는 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\mathbb{E}_{\\pi} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t} \\mid S_0 = s \\right]\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "- $V^{\\pi}(s)$: 정책 $\\pi$에 따른 상태 $s$의 가치.\n",
    "- $\\mathbb{E}_{\\pi}$: `정책` $\\pi$에 따라 기대값을 계산.\n",
    "- $\\gamma$: 할인율 (0 ≤ $\\gamma$ < 1), 미래 보상의 현재 가치 반영.\n",
    "- $R_t$: $t$ 시점에서의 보상.\n",
    "- $S_0 = s$: 현재 상태가 $s$에서 시작됨을 의미.\n",
    "\n",
    "## 역할\n",
    "상태 가치 함수는 강화학습 에이전트가 **장기적인 보상**을 극대화하는 최적의 정책을 학습할 수 있도록 돕습니다. $V(s)$가 높은 상태일수록 해당 상태에 있을 때 앞으로 받을 보상이 높을 것으로 기대되므로, 에이전트는 이러한 상태를 선호하게 됩니다.\n",
    "\n",
    "## 벨만 방정식\n",
    "상태 가치 함수는 벨만 방정식에 의해 재귀적으로 정의될 수 있습니다. 벨만 방정식은 특정 상태 $s$에서의 가치가 해당 상태에서의 보상과 그 이후의 상태들의 가치의 합으로 표현된다는 것을 보여줍니다:\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = \\sum_{a} \\pi(a | s) \\sum_{s'} P(s' | s, a) \\left[ R(s, a, s') + \\gamma V^{\\pi}(s') \\right]\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "- $\\pi(a | s)$: 상태 $s$에서 행동 $a$를 선택할 확률.\n",
    "- $P(s' | s, a)$: 상태 $s$에서 행동 $a$를 수행한 후 상태 $s'$로 전이될 확률.\n",
    "- $R(s, a, s')$: 상태 $s$에서 행동 $a$를 취해 상태 $s'$로 전이될 때 받는 보상.\n",
    "\n",
    "## 응용\n",
    "상태 가치 함수는 다양한 강화학습 알고리즘에서 사용됩니다:\n",
    "- **정책 평가**: 주어진 정책의 성능을 평가하기 위해 사용.\n",
    "- **정책 개선**: 정책의 성능을 높이기 위해 상태 가치 함수를 기준으로 행동을 선택.\n",
    "- **벨만 최적 방정식**을 통해 최적의 정책을 구할 때 활용.\n",
    "\n",
    "상태 가치 함수는 **정책의 성능을 수량화**하고, 에이전트가 환경에서 최적의 경로를 학습하는 데 중요한 정보를 제공합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
