{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _**GoogLeNet_voc2012**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. inception 모듈\n",
    "2. auxiliary classifier \n",
    "3. global average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs) -> None:\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj) -> None:\n",
    "        super(Inception, self).__init__()\n",
    "        self.branch1 = ConvBlock(in_channels, n1x1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBlock(in_channels, n3x3_reduce, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(n3x3_reduce, n3x3, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBlock(in_channels, n5x5_reduce, kernel_size=1, stride=1, padding=0),\n",
    "            ConvBlock(n5x5_reduce, n5x5, kernel_size=5, stride=1, padding=2))\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            ConvBlock(in_channels, pool_proj, kernel_size=1, stride=1, padding=0))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        return torch.cat([x1, x2, x3, x4], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8635,  0.4031, -0.0955,  ...,  0.3380, -0.8318,  0.0225],\n",
       "          [ 0.2375, -1.1175,  0.5509,  ..., -0.0074,  0.1201, -0.7904],\n",
       "          [-0.4690,  0.0379, -1.0364,  ...,  0.1150, -0.4090,  0.4462],\n",
       "          ...,\n",
       "          [-0.4487,  0.1758, -0.8509,  ..., -0.8205, -0.9048,  0.2880],\n",
       "          [ 1.1155, -0.2393,  1.1096,  ...,  1.4236,  0.2564, -0.4669],\n",
       "          [ 1.9245, -1.4698,  1.0380,  ...,  0.6034, -0.1941,  0.0756]],\n",
       "\n",
       "         [[ 0.5930,  0.4838, -0.9952,  ..., -0.0972,  0.0205,  0.8000],\n",
       "          [-1.6151, -0.1978,  1.1477,  ..., -0.1319, -0.2770,  0.0205],\n",
       "          [ 0.7238, -0.0265, -0.8581,  ..., -0.4399, -1.3675, -0.0685],\n",
       "          ...,\n",
       "          [-1.6555,  1.3695,  1.0087,  ...,  0.5884, -2.3705, -0.1487],\n",
       "          [-0.1795, -0.8887, -2.0101,  ..., -0.3503,  1.3519, -0.4294],\n",
       "          [ 0.4794, -1.1205, -0.8351,  ..., -0.4221,  0.5501, -0.1000]],\n",
       "\n",
       "         [[-0.1722, -1.6042,  0.3616,  ..., -0.3282, -0.7696, -0.0505],\n",
       "          [ 2.2464, -1.0506,  0.4327,  ...,  0.5518, -1.8119, -2.2500],\n",
       "          [-0.4337, -1.5341, -1.0559,  ..., -0.7360, -0.7302, -0.5064],\n",
       "          ...,\n",
       "          [ 0.1241,  0.5185, -0.4813,  ..., -0.2008,  0.5757,  1.0514],\n",
       "          [ 0.4792,  0.6183, -0.4471,  ..., -0.6807, -0.2808,  1.5518],\n",
       "          [ 0.2455, -1.5571, -1.1274,  ...,  1.6467, -0.4175,  0.3717]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.6172, -0.7127, -0.9344,  ...,  1.9913, -0.9689, -0.1717],\n",
       "          [-0.7014,  1.8840,  0.8734,  ...,  0.2849,  0.5462,  0.2858],\n",
       "          [ 1.2248, -1.1839, -2.0312,  ..., -0.3772,  1.0889,  1.1827],\n",
       "          ...,\n",
       "          [-0.3658, -0.5364,  0.1491,  ..., -1.5726, -0.1947, -0.5681],\n",
       "          [-0.1821,  1.0262, -0.6816,  ...,  0.1531, -0.8068, -0.1904],\n",
       "          [ 1.1635, -0.8437,  0.6163,  ..., -1.0227, -0.5864, -0.2871]],\n",
       "\n",
       "         [[-0.8644, -1.7886, -0.8826,  ..., -0.8113, -1.2534,  0.3502],\n",
       "          [-1.2138,  0.4413, -1.6843,  ..., -0.1050, -0.2114,  0.5092],\n",
       "          [-1.0993,  1.6210, -0.8613,  ..., -1.4110,  0.0918, -0.2164],\n",
       "          ...,\n",
       "          [ 0.9078,  0.5013, -1.6432,  ...,  1.9093, -0.8118,  0.2156],\n",
       "          [-0.5405, -0.4063,  1.2465,  ...,  0.4745,  0.1090, -1.5177],\n",
       "          [ 0.0076,  0.6554, -2.2059,  ...,  2.3810,  0.4975, -1.9237]],\n",
       "\n",
       "         [[-0.3365, -0.3087, -1.9926,  ...,  0.1400, -0.2219, -0.2388],\n",
       "          [-0.8210,  2.2132,  0.8244,  ...,  0.2077, -1.1699,  0.1146],\n",
       "          [ 0.3792,  0.4128, -1.1704,  ..., -0.2250, -1.0547,  0.6975],\n",
       "          ...,\n",
       "          [ 1.6005, -0.0310, -0.8706,  ...,  2.0001, -1.5242,  0.6873],\n",
       "          [ 0.2552,  0.4924,  1.4014,  ..., -0.3256,  1.4644, -0.2309],\n",
       "          [ 0.2162,  0.0246,  1.4659,  ..., -0.5244,  1.8490,  0.1062]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2588, -1.7734, -0.3540,  ..., -0.8543,  0.1989,  1.3112],\n",
       "          [-0.3922, -0.4728, -0.0532,  ..., -1.0038, -0.5415, -0.0473],\n",
       "          [ 1.0988,  1.6238,  1.8205,  ...,  0.3887, -0.0388, -0.6496],\n",
       "          ...,\n",
       "          [-0.5738,  0.4322,  0.3431,  ..., -1.4316,  0.3515, -2.5866],\n",
       "          [ 2.9049, -1.7951,  0.2947,  ..., -0.0485,  0.0328,  0.0188],\n",
       "          [-0.9264, -0.6889,  0.2808,  ..., -0.1473,  0.3905, -0.6174]],\n",
       "\n",
       "         [[ 1.9053, -0.0061, -0.3158,  ..., -0.8606, -1.1961,  0.1482],\n",
       "          [ 0.7043,  0.3095, -0.5614,  ..., -1.3689, -0.1463,  0.6918],\n",
       "          [ 0.6868,  0.6620, -0.6690,  ...,  1.0831, -0.1670, -1.6464],\n",
       "          ...,\n",
       "          [ 1.5285, -0.6375,  1.3529,  ..., -0.7337, -2.0420, -0.1099],\n",
       "          [-0.3842, -0.8806, -0.1208,  ..., -1.4760,  1.3165,  0.4303],\n",
       "          [ 0.7956, -0.6146, -0.7067,  ...,  0.0045, -0.3216, -0.4931]],\n",
       "\n",
       "         [[-1.9659, -0.4924, -1.1997,  ..., -1.5771,  1.7660,  0.4939],\n",
       "          [-0.8891,  1.2114,  0.6489,  ..., -0.5307,  0.4925,  1.5667],\n",
       "          [-1.0625,  1.8044, -0.2266,  ..., -1.1842,  2.2868,  1.0498],\n",
       "          ...,\n",
       "          [-0.8052, -0.9380,  2.4173,  ..., -0.0820,  1.2307,  0.6480],\n",
       "          [-0.6145,  1.2828,  0.4662,  ...,  2.9784,  1.1819,  1.1870],\n",
       "          [ 0.8811, -1.3423, -0.1121,  ...,  0.8973, -0.3114, -0.4767]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0655,  0.6353, -1.0026,  ..., -1.6452,  0.3462,  0.3678],\n",
       "          [-0.5033,  0.0820, -2.0311,  ...,  1.5427,  1.1684,  1.2080],\n",
       "          [ 0.7789, -0.4197, -1.3573,  ..., -0.3000, -0.0119,  0.1519],\n",
       "          ...,\n",
       "          [ 1.1804, -1.6539,  0.7623,  ..., -0.2218,  0.1406, -1.3898],\n",
       "          [-0.4900, -3.8295, -0.3911,  ..., -0.3084, -2.3492, -1.6799],\n",
       "          [-0.7312, -0.4084,  0.1950,  ..., -0.4167, -1.2024, -0.0145]],\n",
       "\n",
       "         [[ 0.9009,  0.0269,  1.0907,  ..., -0.0472,  0.6310, -0.6961],\n",
       "          [-0.3108,  0.5089, -0.7827,  ..., -1.6339, -0.6896, -2.9975],\n",
       "          [ 1.0859,  0.1546, -0.2355,  ..., -0.4043,  1.2361, -0.9647],\n",
       "          ...,\n",
       "          [ 0.3064,  0.8129,  1.4049,  ...,  0.1836,  0.7428, -0.3178],\n",
       "          [-0.5003,  1.4506, -1.2757,  ..., -0.1873, -0.4082,  0.8326],\n",
       "          [ 1.1986, -0.1637, -0.5005,  ...,  0.8253,  2.1722, -1.6944]],\n",
       "\n",
       "         [[ 0.6806,  0.2751, -1.6561,  ..., -0.0091,  0.4031,  0.5390],\n",
       "          [-1.2584, -0.4130,  0.3668,  ...,  1.3315,  0.3373,  1.3209],\n",
       "          [ 1.1164,  1.0958, -0.5045,  ..., -0.2352, -0.7246, -1.7693],\n",
       "          ...,\n",
       "          [-0.4800, -0.9807, -1.3089,  ...,  2.0452,  1.8195, -0.7341],\n",
       "          [ 1.1183, -0.5992, -0.7741,  ...,  1.5465, -1.2256,  0.2003],\n",
       "          [ 0.3862,  0.2470,  0.5712,  ..., -0.0206, -0.9841, -0.3517]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예시 입력 텐서 (배치 크기 2, 채널 3, 높이 32, 너비 32)\n",
    "a = torch.randn(2, 3, 32, 32)  # 크기: (2, 3, 32, 32)\n",
    "b = torch.randn(2, 4, 32, 32)  # 크기: (2, 3, 32, 32)\n",
    "torch.cat([a,b], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서 크기:  torch.Size([2, 3, 32, 32])\n",
      "출력 텐서 크기:  torch.Size([2, 256, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(2, 3, 32, 32)\n",
    "\n",
    "# Inception 모델 초기화\n",
    "model = Inception(in_channels=3, n1x1=64, n3x3_reduce=64, n3x3=128, n5x5_reduce=32, n5x5=32, pool_proj=32)\n",
    "\n",
    "# 출력\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"입력 텐서 크기: \", input_tensor.shape)\n",
    "print(\"출력 텐서 크기: \", output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Output Shape:  torch.Size([1, 1, 5, 5])\n",
      "AvgPool2d Output Shape:  torch.Size([1, 3, 4, 4])\n",
      "Conv2d Output Shape:  torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=2, stride=1, padding=0)\n",
    "input_tensor = torch.randn(1, 3, 6, 6)\n",
    "output_tensor = conv(input_tensor)\n",
    "\n",
    "print(\"Conv2d Output Shape: \", output_tensor.shape)\n",
    "\n",
    "pool = nn.AvgPool2d(kernel_size=3, stride=1) # 입력차원과 아웃차원을 명시안함\n",
    "output_tensor = pool(input_tensor)\n",
    "print(\"AvgPool2d Output Shape: \", output_tensor.shape)\n",
    "\n",
    "output_tensor = conv(output_tensor)\n",
    "print(\"Conv2d Output Shape: \", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes) -> None:\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=5, stride=3)\n",
    "        self.conv = ConvBlock(in_channels, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes) -> None:\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 출력 크기를 (1,1)로 고정\n",
    "        self.conv = ConvBlock(in_channels, 128, kernel_size=1, stride=1, padding=0)\n",
    "        self.fc1 = None  # 이후 forward에서 동적으로 생성\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.avgpool(x)  # (B, C, 1, 1)로 변환\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)  # (B, C*1*1) -> (B, C)\n",
    "\n",
    "        if self.fc1 is None:  # 첫 실행 시 동적으로 생성\n",
    "            self.fc1 = nn.Linear(x.shape[1], 1024).to(x.device)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self, aux_logits=True, num_classes=1000) -> None:\n",
    "        # 네트워크 초기화 함수\n",
    "        super(GoogLeNet, self).__init__()\n",
    "\n",
    "        # aux_logits이 True이면 보조 분류기 사용, False이면 사용 안함\n",
    "        assert aux_logits == True or aux_logits == False # optrion을 두가지로 고정\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        # 첫 번째 컨볼루션 블록 (64 채널, 커널 크기 7x7, 스트라이드 2)\n",
    "        self.conv1 = ConvBlock(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        # 첫 번째 맥스풀링 (커널 크기 3x3, 스트라이드 2)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)\n",
    "        # 두 번째 컨볼루션 블록 (64 채널, 커널 크기 1x1)\n",
    "        self.conv2 = ConvBlock(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0)\n",
    "        # 세 번째 컨볼루션 블록 (192 채널, 커널 크기 3x3)\n",
    "        self.conv3 = ConvBlock(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1)\n",
    "        # 두 번째 맥스풀링\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n",
    "\n",
    "        # Inception 블록 (세부적인 채널, 커널 크기 등은 Inception 모듈에서 설정)\n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        # 세 번째 맥스풀링\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "        \n",
    "        # 4번부터 5번까지 여러 Inception 블록 (GoogLeNet의 핵심)\n",
    "        self.a4 = Inception(480, 192, 96, 208, 16, \n",
    "                            48, 64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, \n",
    "                            64, 64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, \n",
    "                            64, 64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, \n",
    "                            64, 64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, \n",
    "                            128, 128)\n",
    "        \n",
    "        # 네 번째 맥스풀링\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # 마지막 Inception 블록\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        # Adaptive 평균 풀링 (출력 크기: 1x1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # 드롭아웃 (확률 0.4)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        # 최종 완전 연결층 (출력 크기: num_classes)\n",
    "        self.linear = nn.Linear(1024, num_classes)\n",
    "\n",
    "        # 보조 분류기 (auxiliary classifiers) 설정\n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = None\n",
    "            self.aux2 = None\n",
    "\n",
    "    def transform_input(self, x: Tensor) -> Tensor:\n",
    "        # 입력 이미지에 대한 정규화 (RGB 채널에 대해 각각 정규화)\n",
    "        x_R = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
    "        x_G = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
    "        x_B = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
    "        x = torch.cat([x_R, x_G, x_B], 1)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # 입력 데이터에 대한 전처리\n",
    "        x = self.transform_input(x)\n",
    "\n",
    "        # 컨볼루션 연산 및 풀링 연산을 순차적으로 적용\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.a4(x)\n",
    "        \n",
    "        # 보조 분류기 1 (훈련 중에만 적용)\n",
    "        aux1: Optional[Tensor] = None # aux1은 Tensor 타입이거나 None일 수 있다.\n",
    "        if self.aux_logits and self.training:\n",
    "            aux1 = self.aux1(x) # aux_logits가 True이고 훈련 중이면 aux1을 self.aux1(x)로 설정\n",
    "\n",
    "        x = self.b4(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.d4(x)\n",
    "\n",
    "        # 보조 분류기 2 (훈련 중에만 적용)\n",
    "        aux2: Optional[Tensor] = None\n",
    "        if self.aux_logits and self.training: # self.traing 모델이 훈련중일때만 작동\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.e4(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.b5(x)\n",
    "        # Adaptive Avg Pooling 적용 (출력 크기: 1x1)\n",
    "        x = self.avgpool(x)\n",
    "        # 텐서 차원을 일렬로 펼침\n",
    "        x = x.view(x.shape[0], -1)  # x = x.reshape(x.shape[0], -1)\n",
    "        # 최종 완전 연결층\n",
    "        x = self.linear(x)\n",
    "        # 드롭아웃 적용\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 훈련 중일 때 보조 분류기 결과 반환, 그렇지 않으면 최종 결과 반환\n",
    "        if self.aux_logits and self.training:\n",
    "            return aux1, aux2\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "라벨: tensor([2, 2, 1, 6, 6, 3, 7, 7, 3, 8, 4, 9, 3, 6, 6, 5, 1, 7, 8, 7, 3, 2, 9, 5,\n",
      "        2, 2, 4, 1, 7, 4, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 이미지 변환 설정_224x224로 resize, 정규화\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# CIFAR-10 데이터셋 다운로드\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader 설정 / 배치사이즈 32\n",
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 데이터셋의 첫 번째 이미지와 라벨 확인\n",
    "data_iter = iter(train_loader)  # DataLoader 객체를 반복 가능한 객체로 변환\n",
    "images, labels = next(data_iter)  # 첫 번째 배치를 가져오기\n",
    "\n",
    "# 라벨 출력\n",
    "print(\"라벨:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 생성\n",
    "def train(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001):\n",
    "    from tqdm import tqdm\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # CUDA 사용 가능하면 GPU 사용, 아니면 CPU 사용\n",
    "    model = model.to(device)  # 모델을 지정된 장치(GPU/CPU)로 이동\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류 손실 함수\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # 모델 학습 및 검증\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()  # 모델을 학습 모드로 전환\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # 미니 배치 단위로 데이터를 불러옴\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # 입력 데이터를 모델에 통과시킴\n",
    "            if model.aux_logits:  # aux_logits가 True일 때\n",
    "                outputs = model(inputs)  # 보조 출력을 포함한 모델 통과\n",
    "                if isinstance(outputs, tuple):  # 모델이 튜플을 반환하면 보조 출력도 포함\n",
    "                    outputs, aux1 = outputs  # 보조 출력이 하나만 반환되는 경우\n",
    "                    aux1 = aux1.view(-1, 10)\n",
    "                    loss1 = criterion(aux1, targets)\n",
    "                    loss = criterion(outputs, targets) + 0.3 * loss1\n",
    "                    # 보조 분류기는 중간 레이어에서 학습을 촉진하는 데 도움이 되며, 이 손실을 주 손실에 추가하여 모델이 더 잘 학습할 수 있도록 만듭니다.\n",
    "                    # 0.3은 하이퍼파라미터로, 보조 분류기의 손실이 주 손실에 미치는 영향을 조절합니다.\n",
    "                else:  # 보조 출력을 하나만 반환할 경우\n",
    "                    outputs = outputs[0]  # 주 출력만 사용\n",
    "                    targets = targets.view(-1)  # (batch_size * height * width,)\n",
    "                    outputs = outputs.view(-1, 10)\n",
    "                    loss = criterion(outputs, targets)\n",
    "            else:  # aux_logits가 False일 때\n",
    "                outputs = model(inputs)\n",
    "                targets = targets.view(-1)  # (batch_size * height * width,)\n",
    "                outputs = outputs.view(-1, 10)  # (batch_size * height * width, num_classes)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # 검증 데이터로 성능 평가\n",
    "        evaluate(model, test_loader, device)\n",
    "\n",
    "# 검증 함수\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 그라디언트 계산 비활성화\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # 입력 데이터를 지정된 장치로 이동\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.view(-1, 10)  # (batch_size * height * width, num_classes)\n",
    "            targets = targets.view(-1)  # (batch_size * height * width,)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [01:24<12:37, 84.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 50.74%\n",
      "Epoch [2/10], Loss: 1.5536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [02:50<11:21, 85.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 58.88%\n",
      "Epoch [3/10], Loss: 1.3574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:16<09:58, 85.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 70.80%\n",
      "Epoch [4/10], Loss: 1.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:39<08:27, 84.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 75.09%\n",
      "Epoch [5/10], Loss: 1.1588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [07:03<07:02, 84.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 77.88%\n",
      "Epoch [6/10], Loss: 1.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [08:28<05:38, 84.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 78.76%\n",
      "Epoch [7/10], Loss: 1.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [09:50<04:11, 83.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 79.81%\n",
      "Epoch [8/10], Loss: 0.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [11:15<02:48, 84.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 81.88%\n",
      "Epoch [9/10], Loss: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [12:40<01:24, 84.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 83.22%\n",
      "Epoch [10/10], Loss: 0.9149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [14:04<00:00, 84.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 83.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성 및 학습\n",
    "model = GoogLeNet(aux_logits=False, num_classes=10)  # CIFAR10에는 10개의 클래스를 사용합니다\n",
    "train(model, train_loader, test_loader, num_epochs=10, learning_rate=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
