{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# 연산 장치 설정\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Tensor를 Image로 시각화하는 함수입니다\n",
    "def show_tensor(tensor):\n",
    "    if (tensor.ndim==4) and (tensor.shape[0]==1):\n",
    "        tensor = tensor.squeeze(0)\n",
    "    num_channels = tensor.shape[0]\n",
    "    if num_channels == 1:\n",
    "        image = tensor.squeeze(0).cpu().numpy()\n",
    "        plt.imshow(image, vmin=0, vmax=20)\n",
    "    elif num_channels == 3:\n",
    "        image = tensor.permute(1,2,0).cpu().numpy()\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        image = torch.argmax(tensor, dim=0).cpu().numpy()\n",
    "        plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image 파이프라인 정의\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    # 각 픽셀이 갖는 값의 범위: 0 ~ 1\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Label 파이프라인 정의\n",
    "label_transform = transforms.Compose([\n",
    "    # DeepLabV1의 Output 해상도에 맞추기 위함입니다\n",
    "    transforms.Resize((32, 32)),\n",
    "    # 각 픽셀이 갖는 값의 범위: 0 ~ 255\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda x: torch.where(x==255, 0, x.long())),\n",
    "])\n",
    "\n",
    "# PASCAL-VOC-2012 데이터 로드\n",
    "dataset = datasets.VOCSegmentation(transform=image_transform,\n",
    "                                   target_transform=label_transform,\n",
    "                                   root='./data', image_set='train', download=True)\n",
    "\n",
    "# 데이터 미니배치 처리                  \n",
    "loader  = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. DeepLabV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV1(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Backbone: Imagenet Pretrained VGG-16\n",
    "        vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "        # Feature Extractor 로드\n",
    "        self.feature1 = torch.nn.ModuleList(list(vgg16.features[:5]))\n",
    "        self.feature2 = torch.nn.ModuleList(list(vgg16.features[5:10]))\n",
    "        self.feature3 = torch.nn.ModuleList(list(vgg16.features[10:17]))\n",
    "        self.feature4 = torch.nn.ModuleList(list(vgg16.features[17:24]))\n",
    "        self.feature5 = torch.nn.ModuleList(list(vgg16.features[24:]))\n",
    "        # Classifier 로드\n",
    "        self.classifier1 = torch.nn.ModuleList(list(vgg16.classifier[:3]))\n",
    "        self.classifier2 = torch.nn.ModuleList(list(vgg16.classifier[3:6]))\n",
    "        self.classifier3 = vgg16.classifier[6]\n",
    "\n",
    "        # ============================  Feature Extractor 수정  ============================ #\n",
    "        # 1. 마지막 두 개의 Pooling Layers 제거\n",
    "        del self.feature4[-1]\n",
    "        del self.feature5[-1]\n",
    "        # 2. 마지막 세 개의 Convolutional Layers --> Atrous Convolutional Layers\n",
    "        self.feature5[0] = torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, padding=2)\n",
    "        self.feature5[2] = torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, padding=2)\n",
    "        self.feature5[4] = torch.nn.Conv2d(512, 512, kernel_size=3, dilation=2, padding=2)\n",
    "        # ================================================================================== #\n",
    "\n",
    "        # ============================     Classifier 수정      ============================ #\n",
    "        # 1. 첫 번째 FC Layer --> Atrous Convolutional Layer\n",
    "        self.classifier1[0] = torch.nn.Conv2d(512, 1024, kernel_size=3, dilation=4, padding=4)\n",
    "        # 2. 나머지 FC Layers --> Convolutional Layers\n",
    "        self.classifier2[0] = torch.nn.Conv2d(1024,1024, kernel_size=1)\n",
    "        self.classifier3    = torch.nn.Conv2d(1024,num_classes, kernel_size=1)\n",
    "        # 3. Dropout 삭제\n",
    "        del self.classifier1[-1]\n",
    "        del self.classifier2[-1]\n",
    "        # ================================================================================== #\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.feature1:    x = layer(x)\n",
    "        for layer in self.feature2:    x = layer(x)\n",
    "        for layer in self.feature3:    x = layer(x)\n",
    "        for layer in self.feature4:    x = layer(x)\n",
    "        for layer in self.feature5:    x = layer(x)\n",
    "        for layer in self.classifier1: x = layer(x)\n",
    "        for layer in self.classifier2: x = layer(x)\n",
    "        return self.classifier3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델: DeepLabV1\n",
    "model = DeepLabV1(num_classes=21)\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저: Momentum + Schedular\n",
    "optimizer = torch.optim.SGD([{'params': list(model.feature1.parameters()) +\n",
    "                                        list(model.feature2.parameters()) +\n",
    "                                        list(model.feature3.parameters()) +\n",
    "                                        list(model.feature4.parameters()) +\n",
    "                                        list(model.feature5.parameters()),   'lr': 0.001},\n",
    "                             {'params': list(model.classifier1.parameters()) +\n",
    "                                        list(model.classifier2.parameters()) +\n",
    "                                        list(model.classifier3.parameters()),'lr': 0.01}],\n",
    "                            momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "\n",
    "# 손실함수: Cross Entropy Loss\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    tqdm_format = f'[Epoch {epoch+1:03d}] {{bar}} {{percentage:3.0f}}% {{remaining}}'\n",
    "    for image, label in tqdm.tqdm(loader, ncols=60, bar_format=tqdm_format):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        # 누적된 그래디언트를 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순전파: 전방 계산\n",
    "        output = model(image)\n",
    "        # 순전파: Loss 계산\n",
    "        loss = criterion(output, label.squeeze(1))\n",
    "        # 역전파: 그래디언트 계산\n",
    "        loss.backward()\n",
    "        # 역전파: 가중치 갱신\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# 가중치 저장\n",
    "torch.save(model.state_dict(), 'v1_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DeepLabV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV2(DeepLabV1):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__(num_classes)\n",
    "\n",
    "        # ============================   ASSP 관련 계층 추가    ============================ #\n",
    "        # Filters at Multiple Sampling Rates\n",
    "        self.conv1 = torch.nn.Conv2d(512, 1024, kernel_size=3, dilation=6, padding=6)\n",
    "        self.conv2 = torch.nn.Conv2d(512, 1024, kernel_size=3, dilation=12,padding=12)\n",
    "        self.conv3 = torch.nn.Conv2d(512, 1024, kernel_size=3, dilation=18,padding=18)\n",
    "        self.conv4 = torch.nn.Conv2d(512, 1024, kernel_size=3, dilation=24,padding=24)\n",
    "        # 1 x 1 Convolution Layer\n",
    "        self.conv  = torch.nn.Conv2d(4096, num_classes, kernel_size=1)\n",
    "        # ================================================================================== #\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.feature1: x = layer(x)\n",
    "        for layer in self.feature2: x = layer(x)\n",
    "        for layer in self.feature3: x = layer(x)\n",
    "        for layer in self.feature4: x = layer(x)\n",
    "        for layer in self.feature5: x = layer(x)\n",
    "        \n",
    "        # Atrous Spatial Pyramid Pooling\n",
    "        x1, x2, x3, x4 = self.conv1(x), self.conv2(x), self.conv3(x), self.conv4(x)\n",
    "\n",
    "        # Sum Fusion\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델: DeepLabV2\n",
    "model = DeepLabV2(num_classes=21)\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저: Momentum + Poly Schedular\n",
    "optimizer = torch.optim.SGD([{'params': list(model.feature1.parameters()) +\n",
    "                                        list(model.feature2.parameters()) +\n",
    "                                        list(model.feature3.parameters()) +\n",
    "                                        list(model.feature4.parameters()) +\n",
    "                                        list(model.feature5.parameters()), 'lr': 0.001},\n",
    "                             {'params': list(model.conv1.parameters()) +\n",
    "                                        list(model.conv2.parameters()) +\n",
    "                                        list(model.conv3.parameters()) +\n",
    "                                        list(model.conv4.parameters()) +\n",
    "                                        list(model.conv.parameters()),     'lr': 0.01}],\n",
    "                            momentum=0.9, weight_decay=5e-4)\n",
    "# Poly Schedular\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: (1 - step/740)**0.9)\n",
    "\n",
    "# 손실함수: Cross Entropy Loss\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    tqdm_format = f'[Epoch {epoch+1:03d}] {{bar}} {{percentage:3.0f}}% {{remaining}}'\n",
    "    for image, label in tqdm.tqdm(loader, ncols=60, bar_format=tqdm_format):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        # 누적된 그래디언트를 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 순전파: 전방 계산\n",
    "        output = model(image)\n",
    "        # 순전파: Loss 계산\n",
    "        loss = criterion(output, label.squeeze(1))\n",
    "        # 역전파: 그래디언트 계산\n",
    "        loss.backward()\n",
    "        # 역전파: 가중치 갱신\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "# 가중치 저장\n",
    "torch.save(model.state_dict(), 'v2_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
