{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 음성 딥러닝의 발전에 대한 핵심 논문과 기법 (연대순 정리)\n",
    "\n",
    "음성 딥러닝 기술은 음성 인식, 음성 합성, 음성 분리 등 다양한 분야에서 급속히 발전했다.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Deep Neural Networks for Acoustic Modeling (2012)**  \n",
    "- **논문:** [\"Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups\"](https://ieeexplore.ieee.org/document/6296526) (Hinton et al., 2012)  \n",
    "- **기법:** 심층 신경망(DNN)을 활용하여 음향 모델링 성능을 크게 향상.  \n",
    "- **영향:** 전통적인 GMM-HMM 기반 모델을 딥러닝 기반 모델로 대체하는 전환점 제공.  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Sequence-to-Sequence Learning with Neural Networks (2014)**  \n",
    "- **논문:** [\"Sequence to Sequence Learning with Neural Networks\"](https://arxiv.org/abs/1409.3215) (Sutskever et al., 2014)  \n",
    "- **기법:** RNN 기반 Seq2Seq 모델. 음성 인식과 같은 시퀀스 변환 문제에서 큰 성공.  \n",
    "- **영향:** 음성 인식과 자연어 처리에서 엔드-투-엔드 접근 방식의 가능성 제시.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **WaveNet: A Generative Model for Raw Audio (2016)**  \n",
    "- **논문:** [\"WaveNet: A Generative Model for Raw Audio\"](https://arxiv.org/abs/1609.03499) (Oord et al., 2016, Google DeepMind)  \n",
    "- **기법:** CNN 기반으로 음성 신호의 파형(raw waveform)을 직접 모델링.  \n",
    "- **영향:** 음성 합성(TTS) 품질을 크게 개선하며, 자연스러운 음성 생성 가능.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Listen, Attend, and Spell (2016)**  \n",
    "- **논문:** [\"Listen, Attend and Spell\"](https://arxiv.org/abs/1508.01211) (Chan et al., 2016)  \n",
    "- **기법:** Attention 메커니즘을 Seq2Seq 모델에 도입.  \n",
    "- **영향:** 음성-텍스트 변환에서 Align-free 모델 개발을 선도.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Tacotron: Towards End-to-End Speech Synthesis (2017)**  \n",
    "- **논문:** [\"Tacotron: Towards End-to-End Speech Synthesis\"](https://arxiv.org/abs/1703.10135) (Wang et al., 2017)  \n",
    "- **기법:** 음성 합성을 위한 End-to-End 모델로, 음향 특성 예측에서 음성 합성까지 통합된 구조.  \n",
    "- **영향:** TTS 시스템의 복잡성을 줄이고 효율성을 증가시킴.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Transformer Models in Speech (2018)**  \n",
    "- **논문:** [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) (Vaswani et al., 2017) → 음성에 적용됨.  \n",
    "- **기법:** RNN을 대체한 Transformer 구조.  \n",
    "- **영향:** 음성 인식과 합성에서 병렬처리 효율성과 성능 향상을 제공.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Conformer: Convolution-augmented Transformer (2020)**  \n",
    "- **논문:** [\"Conformer: Convolution-augmented Transformer for Speech Recognition\"](https://arxiv.org/abs/2005.08100) (Gulati et al., 2020)  \n",
    "- **기법:** Transformer와 CNN을 결합하여 로컬(단기)과 글로벌(장기) 패턴을 효과적으로 모델링.  \n",
    "- **영향:** ASR(Automatic Speech Recognition)에서 SOTA(State-of-the-Art) 성능 달성.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Whisper: Robust Speech Recognition (2022)**  \n",
    "- **논문:** 공식 논문 없음 (오픈소스 모델로 발표)  \n",
    "- **리소스:** [Whisper GitHub 페이지](https://github.com/openai/whisper)  \n",
    "- **기법:** 대규모 데이터로 사전 학습된 ASR 모델.  \n",
    "- **영향:** 다국어 음성 인식, 잡음에 강한 성능 제공.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Diffusion Models for Speech (2023)**  \n",
    "- **논문:** [\"Diffusion Probabilistic Models for Speech Synthesis\"](https://arxiv.org/abs/2107.10361)  \n",
    "- **기법:** 확산 모델(Diffusion Model)을 음성 합성에 활용.  \n",
    "- **영향:** WaveNet 이후로 음성 합성 품질에서 또 다른 도약을 가져옴.\n",
    "\n",
    "---\n",
    "\n",
    "## 결론\n",
    "음성 딥러닝 기술은 데이터 및 모델의 확장과 함께 빠르게 발전하고 있다. 특히 End-to-End 접근 방식, Transformer 기반 모델, 그리고 최근의 Diffusion 모델은 음성 기술을 더 정교하고 효율적으로 만들고 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 음성 인식 및 화자 인식: 문맥/화자 의존성에 따른 정리\n",
    "\n",
    "음성 및 화자 인식은 \"문맥\"과 \"화자\" 의존성에 따라 여러 방식으로 나뉩니다. 이를 간단히 정리하면 아래와 같다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **문맥 의존성과 문맥 독립성**\n",
    "\n",
    "### (1) 문맥종속형 음성 인식 (Context-Dependent Speech Recognition)  \n",
    "- **정의:** 특정 문맥이나 이전에 발화된 단어를 고려하여 음성을 인식.  \n",
    "- **특징:** 발화의 문맥적 흐름, 연속된 단어들의 상관관계를 반영.  \n",
    "- **장점:** 자연스러운 문장 구조와 더 높은 정확도 제공.  \n",
    "- **사용 예시:**  \n",
    "  - 대화형 AI 시스템 (예: Siri, Alexa).  \n",
    "  - 문맥 기반 자막 생성.  \n",
    "\n",
    "### (2) 문맥독립형 음성 인식 (Context-Independent Speech Recognition)  \n",
    "- **정의:** 각 단어를 독립적으로 인식, 문맥 정보는 고려하지 않음.  \n",
    "- **특징:** 단어 수준에서의 단순한 음성-텍스트 변환.  \n",
    "- **장점:** 특정 문맥에 의존하지 않아 빠르고 간단함.  \n",
    "- **사용 예시:**  \n",
    "  - 키워드 기반 음성 명령 시스템 (예: \"불 켜\").  \n",
    "  - 단어별 음성 데이터 분석.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. **화자 인식의 유형**\n",
    "\n",
    "### (1) 화자종속 음성 인식 (Speaker-Dependent Speech Recognition)  \n",
    "- **정의:** 특정 화자를 대상으로 학습된 음성 인식 시스템.  \n",
    "- **특징:** 특정 사용자에 맞춘 음향 모델링으로 높은 정확도 제공.  \n",
    "- **장점:** 사용자 개인화, 높은 정확도.  \n",
    "- **단점:** 다른 화자에게 적용하기 어려움.  \n",
    "- **사용 예시:**  \n",
    "  - 개인화된 음성 명령 시스템 (예: 사용자 맞춤 명령).  \n",
    "  - 특정 화자의 음성 분석 연구.  \n",
    "\n",
    "### (2) 화자독립 음성 인식 (Speaker-Independent Speech Recognition)  \n",
    "- **정의:** 화자와 상관없이 일반적인 음성을 인식하는 시스템.  \n",
    "- **특징:** 다양한 화자를 처리할 수 있도록 대규모 데이터로 학습.  \n",
    "- **장점:** 범용성, 다양한 사용자에게 적합.  \n",
    "- **단점:** 특정 화자의 억양, 발음에 따라 성능 저하 가능.  \n",
    "- **사용 예시:**  \n",
    "  - 공공 서비스용 음성 시스템 (예: 콜센터 자동화).  \n",
    "  - 다국어 음성 비서.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **화자 인식 세부 유형**\n",
    "\n",
    "### (1) 화자식별 (Speaker Identification)  \n",
    "- **정의:** 음성을 통해 발화자가 누구인지 식별하는 기술.  \n",
    "- **특징:** 다수의 등록된 화자 중에서 화자를 구별.  \n",
    "- **사용 예시:**  \n",
    "  - 다중 사용자 시스템에서의 사용자 분리.  \n",
    "  - 보안 시스템에서 발화자 확인.  \n",
    "\n",
    "### (2) 화자검증 (Speaker Verification)  \n",
    "- **정의:** 특정 화자의 음성을 확인하여 인증 여부를 판단하는 기술.  \n",
    "- **특징:** 등록된 화자와 음성이 일치하는지 여부만 확인.  \n",
    "- **사용 예시:**  \n",
    "  - 음성 기반 보안 시스템 (예: 음성 비밀번호).  \n",
    "  - 은행 및 금융 서비스의 음성 인증.  \n",
    "\n",
    "---\n",
    "\n",
    "## 비교표\n",
    "\n",
    "| 구분         | 문맥종속형            | 문맥독립형            | 화자종속              | 화자독립              | 화자식별            | 화자검증            |\n",
    "|--------------|----------------------|----------------------|----------------------|----------------------|----------------------|----------------------|\n",
    "| **정의**     | 문맥 고려한 인식      | 문맥 고려하지 않음     | 특정 화자 대상        | 모든 화자 대상        | 발화자 구별         | 발화자 인증         |\n",
    "| **장점**     | 높은 자연스러움       | 간단, 빠름            | 높은 정확도           | 범용성                | 다수 화자 구별       | 보안 강화           |\n",
    "| **단점**     | 구현 복잡도           | 문맥 정보 부족         | 다른 화자에게 부적합  | 특정 화자 성능 저하   | 다수 사용자 필수     | 등록 화자 제한       |\n",
    "| **사용 예시**| 대화형 AI             | 키워드 명령           | 개인 음성 명령        | 공공 서비스           | 사용자 구별 시스템   | 보안 인증           |\n",
    "\n",
    "---\n",
    "\n",
    "이러한 기술들은 음성 인식 및 화자 인증 분야에서 사용자 경험과 보안 수준을 높이는 데 활용되고 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSVQ (Multi-Section Vector Quantization) 개념, 방법론, 그리고 Python 구현 예시\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **MSVQ 개념**\n",
    "\n",
    "**Multi-Section Vector Quantization(MSVQ)**는 고차원 데이터를 효율적으로 압축하기 위한 기법입니다.  \n",
    "일반적인 벡터 양자화(Vector Quantization, VQ)는 고차원 벡터 공간을 코드북(codebook)으로 분할하여 데이터를 양자화하는 방식입니다.  \n",
    "MSVQ는 이를 여러 \"섹션(section)\"으로 나누어 각 섹션별로 별도의 코드북을 사용하여 양자화를 수행합니다.  \n",
    "\n",
    "### 주요 특징:\n",
    "- **분할 기반 처리**: 데이터를 여러 섹션으로 나누어 처리하므로 계산 복잡도가 감소.\n",
    "- **효율적인 저장**: 각 섹션에 대해 별도의 코드북을 학습, 메모리 사용량 감소.\n",
    "- **에러 감소**: 고차원 데이터를 여러 차원으로 분할하면 양자화 에러가 줄어드는 경향이 있음.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **MSVQ 방법론**\n",
    "\n",
    "### (1) **데이터 분할**\n",
    "고차원 벡터를 여러 섹션(차원 부분집합)으로 분리합니다.  \n",
    "예를 들어, 벡터 $( \\mathbf{x} \\in \\mathbb{R}^{d} $)가 있다면 이를 $( \\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_k $)로 나눕니다.\n",
    "\n",
    "### (2) **섹션별 코드북 생성**\n",
    "- 각 섹션에 대해 독립적으로 벡터 양자화를 수행하여 코드북 생성.\n",
    "- 코드북은 K-Means와 같은 클러스터링 알고리즘을 통해 학습.\n",
    "\n",
    "### (3) **양자화**\n",
    "- 입력 벡터를 섹션별로 분리한 후, 각 섹션에서 가장 가까운 코드북 벡터를 선택하여 재구성.\n",
    "\n",
    "### (4) **압축 및 복원**\n",
    "- 양자화된 코드북 인덱스를 저장하여 데이터 압축.\n",
    "- 복원 시 코드북과 저장된 인덱스를 사용하여 원 데이터를 재구성.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **MSVQ의 장단점**\n",
    "\n",
    "### 장점:\n",
    "1. 계산 복잡도를 줄이고, 고차원 데이터를 효율적으로 처리 가능.\n",
    "2. 섹션별 처리로 양자화 에러가 줄어드는 경향이 있음.\n",
    "3. 큰 코드북 대신 여러 작은 코드북 사용으로 메모리 절약.\n",
    "\n",
    "### 단점:\n",
    "1. 섹션 분할에 따라 성능이 민감하게 변함.\n",
    "2. 각 섹션 간 독립성이 전제되어야 최적의 성능 발휘.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **MSVQ Python 구현 예시**\n",
    "\n",
    "아래는 MSVQ 알고리즘의 Python 구현 예시입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'config_context' from 'sklearn.utils' (c:\\Users\\hopio\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     72\u001b[0m msvq \u001b[38;5;241m=\u001b[39m MSVQ(sections, n_clusters)\n\u001b[1;32m---> 73\u001b[0m msvq\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m     75\u001b[0m quantized_indices \u001b[38;5;241m=\u001b[39m msvq\u001b[38;5;241m.\u001b[39mquantize(data)\n\u001b[0;32m     76\u001b[0m reconstructed_data \u001b[38;5;241m=\u001b[39m msvq\u001b[38;5;241m.\u001b[39mreconstruct(quantized_indices)\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mMSVQ.fit\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m         kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'config_context' from 'sklearn.utils' (c:\\Users\\hopio\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# OpenBLAS 환경 변수 설정 (Windows용)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "class MSVQ:\n",
    "    def __init__(self, sections, n_clusters):\n",
    "        self.sections = sections\n",
    "        self.n_clusters = n_clusters\n",
    "        self.codebooks = []\n",
    "\n",
    "    def fit(self, data):\n",
    "        from sklearn.utils import config_context\n",
    "\n",
    "        with config_context(assume_finite=True):\n",
    "            kmeans = KMeans(n_clusters=self.n_clusters, n_init=10, random_state=42)\n",
    "            kmeans.fit(section_data)\n",
    "        \n",
    "        n_samples, n_features = data.shape\n",
    "        assert n_features % self.sections == 0, \"섹션 수는 데이터 차원을 나눌 수 있어야 합니다.\"\n",
    "\n",
    "        section_size = n_features // self.sections\n",
    "        self.codebooks = []\n",
    "\n",
    "        for i in range(self.sections):\n",
    "            start = i * section_size\n",
    "            end = start + section_size\n",
    "            section_data = data[:, start:end]\n",
    "\n",
    "            # K-Means로 코드북 생성 (n_init 파라미터 추가)\n",
    "            kmeans = KMeans(n_clusters=self.n_clusters, n_init=10, random_state=42)\n",
    "            kmeans.fit(section_data)\n",
    "            self.codebooks.append(kmeans.cluster_centers_)\n",
    "\n",
    "    def quantize(self, data):\n",
    "        n_samples, n_features = data.shape\n",
    "        section_size = n_features // self.sections\n",
    "        quantized_indices = []\n",
    "\n",
    "        for i in range(self.sections):\n",
    "            start = i * section_size\n",
    "            end = start + section_size\n",
    "            section_data = data[:, start:end]\n",
    "\n",
    "            codebook = self.codebooks[i]\n",
    "            distances = np.linalg.norm(section_data[:, np.newaxis] - codebook, axis=2)\n",
    "            quantized_indices.append(np.argmin(distances, axis=1))\n",
    "\n",
    "        return quantized_indices\n",
    "\n",
    "    def reconstruct(self, quantized_indices):\n",
    "        reconstructed_data = []\n",
    "\n",
    "        for i, indices in enumerate(quantized_indices):\n",
    "            codebook = self.codebooks[i]\n",
    "            reconstructed_section = codebook[indices]\n",
    "            reconstructed_data.append(reconstructed_section)\n",
    "\n",
    "        return np.hstack(reconstructed_data)\n",
    "\n",
    "\n",
    "# 테스트\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    data = np.random.rand(100, 8)\n",
    "\n",
    "    sections = 2\n",
    "    n_clusters = 4\n",
    "\n",
    "    msvq = MSVQ(sections, n_clusters)\n",
    "    msvq.fit(data)\n",
    "\n",
    "    quantized_indices = msvq.quantize(data)\n",
    "    reconstructed_data = msvq.reconstruct(quantized_indices)\n",
    "\n",
    "    print(\"원본 데이터:\\n\", data[:5])\n",
    "    print(\"복원된 데이터:\\n\", reconstructed_data[:5])\n",
    "    print(\"양자화 에러 (MSE):\", np.mean((data - reconstructed_data) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDNN (Time-Delay Neural Network)\n",
    "**TDNN (Time-Delay Neural Network)**은 시간적 특성을 고려하여 시퀀스 데이터를 처리하는 데 강점을 가진 신경망 모델입니다. 주로 음성 인식, 오디오 분석, 시계열 데이터 처리에 사용됩니다. TDNN은 시간 지연(Time Delay)을 모델링하여, 데이터를 순차적으로 처리하는 데 유용하하다.\n",
    "\n",
    "- 1. 기본 개념\n",
    "TDNN은 전통적인 다층 퍼셉트론(MLP) 구조에서 발전된 형태로, 입력 벡터에 대해 시간 지연된 정보를 처리할 수 있도록 설계되었습니다. 각 계층에서 일정한 시간 간격을 두고 데이터를 처리할 수 있기 때문에 시계열이나 음성 신호와 같은 순차적인 패턴을 잘 인식할 수 있습니다.\n",
    "\n",
    "- 2. 구조\n",
    "TDNN의 주요 특징은 시간 지연을 도입한 것이며, 이를 통해 이전 시간 단계의 정보를 모델링할 수 있습니다. 예를 들어, TDNN은 입력 벡터에서 특정 시간 지점에 대해 여러 과거의 데이터를 동시에 고려하여 학습을 진행할 수 있습니다.\n",
    "\n",
    "기본적인 TDNN의 구조는 다음과 같습니다:\n",
    "\n",
    "    - 입력 레이어: 시퀀스 데이터를 시간 지연을 고려하여 입력합니다.\n",
    "    - 시간 지연 계층 (Time Delay Layers): 이전 시간의 정보를 포함하여 시퀀스 데이터를 처리합니다.\n",
    "    - 출력 레이어: 최종적인 예측 값을 출력합니다.\n",
    "\n",
    "- 3. 장점\n",
    "시간적 의존성 모델링: TDNN은 시퀀스 데이터를 처리할 때 중요한 시간적 관계를 잘 모델링할 수 있습니다.\n",
    "효율적인 계산: 시간 지연을 모델링하는 방식으로, RNN보다 계산 효율성이 높은 경우가 많습니다.\n",
    "\n",
    "- 4. 활용 사례\n",
    "음성 인식: TDNN은 음성 인식 시스템에서 널리 사용됩니다. 시간 지연을 이용해 음성의 패턴을 인식하고, 시간적 변화를 반영할 수 있습니다.\n",
    "시계열 데이터 분석: 주식 가격 예측, 기후 변화 예측 등 시계열 데이터를 처리할 때 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.0515\n",
      "Epoch [2/10], Loss: 1.0288\n",
      "Epoch [3/10], Loss: 1.0141\n",
      "Epoch [4/10], Loss: 1.0061\n",
      "Epoch [5/10], Loss: 1.0043\n",
      "Epoch [6/10], Loss: 1.0042\n",
      "Epoch [7/10], Loss: 1.0039\n",
      "Epoch [8/10], Loss: 1.0021\n",
      "Epoch [9/10], Loss: 0.9992\n",
      "Epoch [10/10], Loss: 0.9965\n",
      "예측값: tensor([[-0.1225]])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 간단한 TDNN 모델 클래스 정의\n",
    "class TDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, kernel_size):\n",
    "        super(TDNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=kernel_size, stride=1)\n",
    "        #! 커널이 \"이전\" 시간 값을 참조하므로 시간 지연이 발생합니다\n",
    "        self.conv2 = nn.Conv1d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=kernel_size, stride=1)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # 첫 번째 1D Convolution\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x)  # 두 번째 1D Convolution\n",
    "        x = torch.relu(x)\n",
    "        x = x.mean(dim=2)  # 평균 풀링\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "input_size = 1  # 입력 시퀀스의 채널 수\n",
    "hidden_size = 64\n",
    "output_size = 1  # 예측 값\n",
    "kernel_size = 3  # 커널 크기 (시간 지연)\n",
    "batch_size = 16\n",
    "\n",
    "# 데이터 예시 (시계열 데이터)\n",
    "# (배치 크기, 채널 수, 시퀀스 길이)\n",
    "data = torch.randn(batch_size, input_size, 100)  # 길이 100의 시퀀스 데이터 예시\n",
    "labels = torch.randn(batch_size, output_size)    # 예측 대상\n",
    "\n",
    "# 모델 및 손실 함수 정의\n",
    "model = TDNN(input_size, hidden_size, output_size, kernel_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 예측\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = torch.randn(1, input_size, 100)  # 테스트 데이터\n",
    "    prediction = model(test_data)\n",
    "    print(f'예측값: {prediction}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
